<!doctype html>
<html lang='pt-BR'>
<head>
<meta charset='utf-8'/>
<meta name='viewport' content='width=device-width, initial-scale=1'/>
<title>Relatório de Documentação TCC</title>
<style>
:root {
  --bg: #0F1722;
  --panel: rgba(15,23,34,0.66);
  --panel2: rgba(15,23,34,0.44);
  --border: rgba(148,163,184,0.18);
  --text: #F5F7FA;
  --muted: rgba(245,247,250,0.7);
  --accent: #00F3FF;
  --accent2: #7B61FF;
  --ok: #00FFA3;
  --warn: #FFB800;
  --bad: #FF2E5F;
  --mono: 'JetBrains Mono', ui-monospace, Menlo, Consolas, monospace;
  --sans: 'Inter', ui-sans-serif, system-ui, -apple-system, sans-serif;
  --radius: 14px;
}
* { box-sizing: border-box; }
body { margin: 0; font-family: var(--sans); color: var(--text); background: radial-gradient(circle at 20% 40%, rgba(0,243,255,.05), transparent 55%), radial-gradient(circle at 80% 10%, rgba(123,97,255,.05), transparent 55%), linear-gradient(180deg, #0b0f14, var(--bg)); }
.container { max-width: 1200px; margin: 0 auto; padding: 24px; }
.header { background: linear-gradient(135deg, rgba(15,23,34,.9), rgba(15,23,34,.65)); border: 1px solid var(--border); border-radius: 18px; padding: 20px 24px; box-shadow: 0 20px 50px rgba(0,0,0,.45); }
.title { font-size: 2.2rem; font-weight: 800; letter-spacing: -0.02em; background: linear-gradient(90deg, var(--accent), var(--accent2)); -webkit-background-clip: text; color: transparent; }
.subtitle { margin-top: 6px; color: var(--muted); }
.badge { display: inline-block; padding: 4px 10px; border-radius: 999px; border: 1px solid var(--border); font-size: .75rem; text-transform: uppercase; letter-spacing: .06em; color: var(--muted); }
.section { margin-top: 22px; padding: 18px 20px; background: var(--panel); border: 1px solid var(--border); border-radius: var(--radius); }
.section h2 { margin: 0 0 12px 0; font-size: 1.4rem; }
.section h3 { margin: 16px 0 8px 0; font-size: 1.1rem; color: #E9EEF5; }
.section p { color: var(--muted); line-height: 1.6; }
.grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(240px,1fr)); gap: 12px; }
.card { background: var(--panel2); border: 1px solid var(--border); border-radius: 12px; padding: 12px; }
.kv { display: flex; justify-content: space-between; gap: 10px; font-size: .92rem; }
.kv span { color: var(--muted); }
pre { background: #0b111b; border: 1px solid rgba(255,255,255,0.08); border-radius: 10px; padding: 12px; overflow: auto; }
code { font-family: var(--mono); font-size: .85rem; color: #E5E7EB; }
.table { width: 100%; border-collapse: collapse; }
.table th, .table td { border-bottom: 1px solid rgba(255,255,255,0.08); padding: 8px 10px; text-align: left; }
.table th { color: var(--accent); font-weight: 600; font-size: .9rem; text-transform: uppercase; letter-spacing: .04em; }
.table td { color: var(--muted); font-size: .92rem; vertical-align: top; }
.toc { margin-top: 14px; }
.toc details { background: var(--panel2); border: 1px solid var(--border); border-radius: 12px; padding: 10px 12px; }
.toc a { color: var(--accent); text-decoration: none; }
.toc li { margin: 6px 0; }
.badge-impl { background: rgba(0,255,163,.12); color: var(--ok); border-color: rgba(0,255,163,.2); }
.badge-plan { background: rgba(255,184,0,.12); color: var(--warn); border-color: rgba(255,184,0,.2); }
.badge-missing { background: rgba(255,46,95,.12); color: var(--bad); border-color: rgba(255,46,95,.2); }
.footer { margin-top: 24px; color: var(--muted); font-size: .85rem; }
</style>
</head>
<body>
<div class='container'>
  <div class='header'>
    <div class='title'>Relatório de Documentação TCC</div>
    <div class='subtitle'>Brazilian Energy Dashboard with ML — documentação técnica detalhada</div>
    <div style='margin-top:10px'>
      <span class='badge'>Gerado em 2026-02-01 16:22:36</span>
      <span class='badge'>Projeto: /home/capponni/Projeto_dashboard</span>
    </div>
  </div>

  <div class='toc'>
    <details open>
      <summary><strong>Índice</strong></summary>
      <ol>
        <li><a href='#1-sum-rio-executivo'>Sumário Executivo</a></li><li><a href='#2-vis-o-geral-do-projeto'>Visão Geral do Projeto</a></li><li><a href='#3-an-lise-da-estrutura-de-pastas'>Análise da Estrutura de Pastas</a></li><li><a href='#4-documenta-o-do-banco-de-dados'>Documentação do Banco de Dados</a></li><li><a href='#5-m-dulos-do-sistema-an-lise-detalhada'>Módulos do Sistema - Análise Detalhada</a></li><li><a href='#6-dashboard-documenta-o-completa'>Dashboard - Documentação Completa</a></li><li><a href='#7-pipeline-de-dados-completo'>Pipeline de Dados Completo</a></li><li><a href='#8-configura-o-e-deployment'>Configuração e Deployment</a></li><li><a href='#9-testes-e-valida-es'>Testes e Validações</a></li><li><a href='#10-an-lise-de-dados-do-mercado-de-energia'>Análise de Dados do Mercado de Energia</a></li><li><a href='#11-modelos-de-machine-learning'>Modelos de Machine Learning</a></li><li><a href='#12-considera-es-t-cnicas-para-tcc'>Considerações Técnicas para TCC</a></li><li><a href='#13-gloss-rio'>Glossário</a></li><li><a href='#14-refer-ncias'>Referências</a></li>
      </ol>
    </details>
  </div>
<div class='section' id='1-sum-rio-executivo'>
<h2>1. Sumário Executivo</h2>

<p>Este relatório consolida a documentação técnica do projeto <strong>Brazilian Energy Dashboard with ML</strong>, com foco em ingestão de dados do setor elétrico, armazenamento em banco relacional, previsões de preços BBCE e visualizações em Streamlit. O conteúdo foi gerado a partir da leitura direta do código-fonte e arquivos de configuração do repositório atual.</p>
<div class='grid'>
  <div class='card'><div class='kv'><strong>Status geral</strong><span>Operacional (dashboard + ingestão)</span></div></div>
  <div class='card'><div class='kv'><strong>Módulo ML</strong><span>Baseline Prophet/SARIMAX</span></div></div>
  <div class='card'><div class='kv'><strong>Banco de dados</strong><span>PostgreSQL/SQLite</span></div></div>
  <div class='card'><div class='kv'><strong>Docs existentes</strong><span>api.md, data_dictionary.md, model_docs.md, user_guide.md, troubleshooting.md</span></div></div>
</div>

</div><div class='section' id='2-vis-o-geral-do-projeto'>
<h2>2. Visão Geral do Projeto</h2>

<p><strong>Propósito:</strong> disponibilizar um painel interativo para análise do cenário elétrico brasileiro, com ingestão de dados BSO/BBCE/PLD/CMO, armazenamento em banco relacional e previsões de preços BBCE.</p>
<p><strong>Arquitetura:</strong> pipeline CLI (main.py) para setup/ingestão/treino/forecast, banco via SQLAlchemy, visualização em Streamlit e assets auxiliares (mapa do Brasil em GeoJSON).</p>
<p><strong>Tecnologias:</strong> Streamlit (UI), Pandas/Numpy (ETL), SQLAlchemy (DB), PostgreSQL/SQLite (armazenamento), statsmodels/Prophet (ML), Plotly (gráficos).</p>
<p><strong>Público-alvo:</strong> analistas e estudantes do setor elétrico (TCC Engenharia Elétrica / Data Science), com ênfase em interpretação de indicadores e preços BBCE.</p>

</div><div class='section' id='3-an-lise-da-estrutura-de-pastas'>
<h2>3. Análise da Estrutura de Pastas</h2>

<p>Abaixo a estrutura real das pastas (profundidade 3):</p>
<pre><code class='language-text'>project_root/
  .env
  .env.example
  .gitignore
  Dockerfile
  README.md
  dashboard.py
  database_setup.sql
  docker-compose.yml
  main.py
  pytest.ini
  requirements.txt
  streamlit.log
  assets/
    brazil-states.geojson
  .venv/
    .gitignore
    CACHEDIR.TAG
    pyvenv.cfg
    etc/
      jupyter/
    lib/
      python3.12/
    include/
      site/
    share/
      man/
      jupyter/
    bin/
      activate
      activate.csh
      activate.fish
      activate.nu
      activate.ps1
      activate_this.py
      dotenv
      f2py
      fonttools
      install_cmdstan
      install_cxx_toolchain
      jsonschema
      normalizer
      numpy-config
      pip
      pip-3.12
      pip3
      pip3.12
      plotly_get_chrome
      py.test
      pyftmerge
      pyftsubset
      pygmentize
      pytest
      python
      python3
      python3.12
      streamlit
      streamlit.cmd
      tqdm
      ttx
      watchmedo
  src/
    __init__.py
    utils/
      __init__.py
      constants.py
      __pycache__/
    data_processing/
      __init__.py
      ingestion.py
      pipeline.py
      validators.py
      __pycache__/
    evaluation/
      __pycache__/
    feature_engineering/
      __pycache__/
    visualization/
      __init__.py
      components.py
      data.py
      ui.py
      __pycache__/
    database/
      __init__.py
      db.py
      ingest.py
      schema.py
      setup.py
      __pycache__/
    __pycache__/
    ml_models/
      __init__.py
      data.py
      predict.py
      train.py
      deep_learning/
      traditional/
      ensemble/
      __pycache__/
      ml/
  data/
    models/
      sarimax_ANU_ABR_25_DEZ_26.pkl
      sarimax_ANU_JAN_25_DEZ_26.pkl
      sarimax_ANU_JAN_25_DEZ_27.pkl
      sarimax_ANU_JAN_25_DEZ_28.pkl
      sarimax_ANU_JAN_26_DEZ_26.pkl
      sarimax_ANU_JAN_26_DEZ_27.pkl
      sarimax_ANU_JAN_26_DEZ_28.pkl
      sarimax_ANU_JAN_26_DEZ_29.pkl
      sarimax_ANU_JAN_26_DEZ_30.pkl
      sarimax_ANU_JAN_27_DEZ_27.pkl
      sarimax_ANU_JAN_28_DEZ_28.pkl
      sarimax_ANU_JAN_28_DEZ_30.pkl
      sarimax_ANU_JAN_29_DEZ_29.pkl
      sarimax_ANU_JAN_30_DEZ_30.pkl
      sarimax_ANU_JAN_31_DEZ_31.pkl
      sarimax_ANU_JAN_31_DEZ_32.pkl
      sarimax_ANU_JAN_32_DEZ_32.pkl
      sarimax_ANU_JAN_33_DEZ_33.pkl
      sarimax_ANU_JAN_34_DEZ_34.pkl
      sarimax_ANU_JAN_35_DEZ_35.pkl
      sarimax_SEM_JAN_26_JUN_26.pkl
      sarimax_SEM_JAN_27_JUN_27.pkl
      sarimax_SEM_JUL_26_DEZ_26.pkl
      sarimax_SEM_JUL_27_DEZ_27.pkl
      sarimax_TRI_ABR_26_JUN_26.pkl
      sarimax_TRI_JAN_26_MAR_26.pkl
      sarimax_TRI_JAN_27_MAR_27.pkl
      sarimax_TRI_JAN_28_MAR_28.pkl
      sarimax_TRI_JUL_26_SET_26.pkl
      sarimax_TRI_JUL_27_SET_27.pkl
      sarimax_TRI_JUL_28_SET_28.pkl
      sarimax_TRI_OUT_26_DEZ_26.pkl
      sarimax_TRI_OUT_27_DEZ_27.pkl
    database/
      energy_dashboard.db
    processed/
    raw_csv/
      ban.csv
      bbce.csv
      bso.csv
      cmo.csv
      pld.csv
  scripts/
    __pycache__/
  config/
    __init__.py
    settings.py
    __pycache__/
  tests/
    test_ingestion.py
    __pycache__/
  .pytest_cache/
    .gitignore
    CACHEDIR.TAG
    README.md
    v/
      cache/
        nodeids
  docs/
    api.md
    data_dictionary.md
    model_docs.md
    troubleshooting.md
    user_guide.md
  .streamlit/
    config.toml
  __pycache__/</code></pre>
<p><strong>Observação:</strong> diretórios <code>src/ml_models/deep_learning</code>, <code>src/ml_models/traditional</code>, <code>src/ml_models/ensemble</code>, <code>src/feature_engineering</code> e <code>src/evaluation</code> estão presentes, porém não possuem arquivos <code>.py</code> (apenas <code>__pycache__</code>), indicando áreas planejadas ou removidas.</p>

</div><div class='section' id='4-documenta-o-do-banco-de-dados'>
<h2>4. Documentação do Banco de Dados</h2>

<p>O esquema é definido em <code>database_setup.sql</code> (PostgreSQL) e replicado em <code>src/database/schema.py</code> para SQLite. As tabelas abaixo refletem o esquema atual:</p>
<h3>bso_data</h3><ul><li><code>id SERIAL PRIMARY KEY</code></li><li><code>bso_week VARCHAR(50)</code></li><li><code>submercado VARCHAR(30)</code></li><li><code>data_inicio DATE</code></li><li><code>data_fim DATE</code></li><li><code>carga DECIMAL(15,6)</code></li><li><code>intercambio DECIMAL(15,6)</code></li><li><code>hidraulica DECIMAL(15,6)</code></li><li><code>termica DECIMAL(15,6)</code></li><li><code>eolica DECIMAL(15,6)</code></li><li><code>solar DECIMAL(15,6)</code></li><li><code>ena DECIMAL(10,2)</code></li><li><code>ear DECIMAL(10,2)</code></li><li><code>itaipu DECIMAL(15,6)</code></li><li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code></li><li><code>UNIQUE(bso_week, submercado)</code></li></ul>
<h3>bbce_data</h3><ul><li><code>id SERIAL PRIMARY KEY</code></li><li><code>produto VARCHAR(50)</code></li><li><code>data DATE</code></li><li><code>preco DECIMAL(10,2)</code></li><li><code>tipo_produto VARCHAR(10)</code></li><li><code>ano_referencia INTEGER</code></li><li><code>mes_inicio INTEGER</code></li><li><code>mes_fim INTEGER</code></li><li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code></li><li><code>UNIQUE(produto, data)</code></li></ul>
<h3>bbce_trades</h3><ul><li><code>id SERIAL PRIMARY KEY</code></li><li><code>produto VARCHAR(50)</code></li><li><code>data DATE</code></li><li><code>preco DECIMAL(10,2)</code></li><li><code>qtd INTEGER</code></li><li><code>tipo_produto VARCHAR(10)</code></li><li><code>ano_referencia INTEGER</code></li><li><code>mes_inicio INTEGER</code></li><li><code>mes_fim INTEGER</code></li><li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code></li><li><code>UNIQUE(produto, data, preco)</code></li></ul>
<h3>ban_data</h3><ul><li><code>id SERIAL PRIMARY KEY</code></li><li><code>competencia DATE</code></li><li><code>bandeira VARCHAR(30)</code></li><li><code>adicional DECIMAL(10,4)</code></li><li><code>data_geracao DATE</code></li><li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code></li><li><code>UNIQUE(competencia)</code></li></ul>
<h3>pld_data</h3><ul><li><code>id SERIAL PRIMARY KEY</code></li><li><code>regiao VARCHAR(30)</code></li><li><code>metrica VARCHAR(20)</code></li><li><code>periodo VARCHAR(20)</code></li><li><code>tipo VARCHAR(20)</code></li><li><code>valor DECIMAL(10,4)</code></li><li><code>data_inicio DATE</code></li><li><code>data_fim DATE</code></li><li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code></li><li><code>UNIQUE(regiao, data_inicio, metrica)</code></li></ul>
<h3>cmo_data</h3><ul><li><code>id SERIAL PRIMARY KEY</code></li><li><code>regiao VARCHAR(30)</code></li><li><code>metrica VARCHAR(20)</code></li><li><code>periodo VARCHAR(20)</code></li><li><code>tipo VARCHAR(20)</code></li><li><code>valor DECIMAL(10,4)</code></li><li><code>data_inicio DATE</code></li><li><code>data_fim DATE</code></li><li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code></li><li><code>UNIQUE(regiao, data_inicio, metrica)</code></li></ul>
<h3>ml_models</h3><ul><li><code>id SERIAL PRIMARY KEY</code></li><li><code>model_name VARCHAR(100)</code></li><li><code>model_type VARCHAR(50)</code></li><li><code>product_type VARCHAR(20)</code></li><li><code>horizon VARCHAR(10)</code></li><li><code>metrics JSONB</code></li><li><code>model_path VARCHAR(255)</code></li><li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code></li><li><code>last_updated TIMESTAMP</code></li></ul>
<h3>predictions</h3><ul><li><code>id SERIAL PRIMARY KEY</code></li><li><code>product VARCHAR(50)</code></li><li><code>prediction_date DATE</code></li><li><code>horizon VARCHAR(10)</code></li><li><code>predicted_value DECIMAL(10,2)</code></li><li><code>confidence_interval_lower DECIMAL(10,2)</code></li><li><code>confidence_interval_upper DECIMAL(10,2)</code></li><li><code>trend VARCHAR(20)</code></li><li><code>model_id INTEGER REFERENCES ml_models(id)</code></li><li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code></li></ul>
<h3>Relações (diagrama textual)</h3>
<pre><code class='language-text'>predictions.model_id -&gt; ml_models.id
(bbce_data, bso_data, pld_data, cmo_data, ban_data) sem FKs explícitas</code></pre>
<p><strong>ETL:</strong> ingestão via <code>src/data_processing</code> e persistência com <code>src/database/ingest.py</code> usando upsert/ignore por conflitos.</p>

</div><div class='section' id='5-m-dulos-do-sistema-an-lise-detalhada'>
<h2>5. Módulos do Sistema - Análise Detalhada</h2>

<h3>src/data_processing/</h3>
<p>Responsável por leitura, normalização e ajustes de domínio. Principais funções:</p>
<ul>
  <li><code>process_bbce</code> e <code>process_bbce_trades</code>: normaliza preços, resolve escala 10x e deriva tipo/ano/mês do produto.</li>
  <li><code>process_bso</code>: ajusta Itaipu no SE/CO e inverte o intercâmbio do Nordeste.</li>
  <li><code>process_ban</code>: bandeiras tarifárias (mensais).</li>
</ul>
<pre><code class='language-python'>def process_bbce(path: Path) -&gt; pd.DataFrame:
    df = normalize_columns(load_csv(path))
    require_columns(df, ["produto", "data", "preco"], "bbce")
    df = coerce_datetime(df, ["data"])
    df = coerce_numeric(df, ["preco"])

    # Data hygiene (BBCE raw often comes with a 10x scaling issue where values appear as 1000-1999.9).
    # Normalize those back to the expected scale before persisting/aggregating.
    mask_10x = df["preco"].between(1000, 2000, inclusive="left")
    df.loc[mask_10x, "preco"] = df.loc[mask_10x, "preco"] / 10.0

    # Drop invalid prices (negative/zero).
    df.loc[df["preco"] &lt;= 0, "preco"] = pd.NA
    df = df.dropna(subset=["preco"])

    details = df["produto"].apply(parse_product_details)
    details_df = pd.DataFrame(list(details))
    df = pd.concat([df, details_df], axis=1)

    df = drop_duplicates(df, ["produto", "data"])
    return df

</code></pre>
<pre><code class='language-python'>def process_bso(path: Path) -&gt; pd.DataFrame:
    df = normalize_columns(load_csv(path))
    require_columns(
        df,
        [
            "bso",
            "submercado",
            "data_inicio",
            "data_fim",
            "carga",
            "intercambio",
            "hidraulica",
            "termica",
            "eolica",
            "solar",
            "ena",
            "ear",
            "itaipu",
        ],
        "bso",
    )

    df = coerce_datetime(df, ["data_inicio", "data_fim"])
    numeric_cols = [
        "carga",
        "intercambio",
        "hidraulica",
        "termica",
        "eolica",
        "solar",
        "ena",
        "ear",
        "itaipu",
    ]
    df = coerce_numeric(df, numeric_cols)

    # Operational week label
    df = df.rename(columns={"bso": "bso_week"})

    # Add Itaipu generation to SE/CO hydro
    se_mask = df["submercado"].str.contains("Sudeste", case=False, na=False)
    df.loc[se_mask, "hidraulica"] = (
        df.loc[se_mask, "hidraulica"].fillna(0) + df.loc[se_mask, "itaipu"].fillna(0)
    )

    # Invert NE interchange sign to keep balance consistent
    ne_mask = df["submercado"].str.contains("Nordeste", case=False, na=False)
    df.loc[ne_mask, "intercambio"] = -df.loc[ne_mask, "intercambio"]

    df = drop_duplicates(df, ["bso_week", "submercado"])
    return df

</code></pre>

<h3>src/database/</h3>
<p>Camada de persistência, com engine SQLAlchemy e ingestão em lote para evitar limites do SQLite.</p>
<pre><code class='language-python'>def insert_dataframe(table_name: str, df: pd.DataFrame, conflict_cols: Iterable[str]) -&gt; int:
    if df.empty:
        return 0

    metadata = MetaData()
    table = Table(table_name, metadata, autoload_with=engine)

    # SQLite is strict about DATE fields: it requires Python `date` objects.
    # Normalize dataframe values based on the table column types.
    df2 = df.copy()
    for col in table.columns:
        if col.name not in df2.columns:
            continue
        if isinstance(col.type, Date):
            df2[col.name] = pd.to_datetime(df2[col.name], errors="coerce").dt.date
        elif isinstance(col.type, DateTime):
            df2[col.name] = pd.to_datetime(df2[col.name], errors="coerce").dt.to_pydatetime()
        elif isinstance(col.type, Integer):
            # Avoid SQLite "cannot convert float NaN to integer".
            s = pd.to_numeric(df2[col.name], errors="coerce")
            s = s.astype("Int64")
            df2[col.name] = s.where(s.notna(), None)

    # Replace any remaining NaN/NaT with None for DBAPI compatibility.
    df2 = df2.where(pd.notna(df2), None)

    records = df2.to_dict(orient="records")
    if not records:
        return 0

    # SQLite has a low max-variable limit (commonly 999). Batch inserts to avoid
    # "too many SQL variables" errors when loading large CSVs like BBCE.
    is_sqlite = engine.dialect.name == "sqlite"
    # Keep a safety margin for the conflict clause / internal bindings.
    max_vars = 900 if is_sqlite else None
    cols_per_row = max(1, len(table.columns) - 1)  # ignore autoincrement id
    batch_size = max(1, (max_vars // cols_per_row)) if max_vars else len(records)

    total = 0
    with engine.begin() as conn:
        for i in range(0, len(records), batch_size):
            chunk = records[i : i + batch_size]
            if is_sqlite:
                stmt = sqlite_insert(table).values(chunk)
            else:
                stmt = pg_insert(table).values(chunk)
            stmt = stmt.on_conflict_do_nothing(index_elements=list(conflict_cols))
            res = conn.execute(stmt)
            total += res.rowcount or 0

    return total

</code></pre>

<h3>src/ml_models/</h3>
<p>Implementa o baseline Prophet/SARIMAX. Treino, avaliação e persistência na tabela <code>ml_models</code>.</p>
<pre><code class='language-python'>def train_sarimax(product: str, model_dir: Path) -&gt; dict[str, str | dict]:
    df = load_bbce(product)
    if df.empty:
        raise RuntimeError(f"No BBCE data available for product {product}")

    y = _prepare_daily_series(df)
    if len(y) &lt; 10:
        last = float(y.iloc[-1]) if len(y) else None
        model_dir.mkdir(parents=True, exist_ok=True)
        model_path = model_dir / f"sarimax_{_safe_filename(product)}.pkl"
        with model_path.open("wb") as handle:
            pickle.dump({"kind": "naive_last", "last": last}, handle)
        return {
            "model_name": f"SARIMAX_{product}",
            "model_type": "naive_last",
            "product_type": df["tipo_produto"].dropna().unique().tolist()[0] if "tipo_produto" in df else None,
            "horizon": "12w",
            "metrics": {"mae": None, "rmse": None, "mape": None},
            "model_path": str(model_path),
        }

    train_y, test_y = _train_test_split(y.reset_index(drop=True).to_frame(name="y"))
    train_series = train_y["y"]
    test_series = test_y["y"]

    model = SARIMAX(
        train_series,
        order=(1, 1, 1),
        seasonal_order=(1, 0, 1, 7),
        enforce_stationarity=False,
        enforce_invertibility=False,
    )
    results = model.fit(disp=False)

    if not test_series.empty:
        forecast = results.get_forecast(steps=len(test_series))
        metrics = _evaluate(test_series.values, forecast.predicted_mean.values)
    else:
        metrics = {"mae": None, "rmse": None, "mape": None}

    model_dir.mkdir(parents=True, exist_ok=True)
    model_path = model_dir / f"sarimax_{_safe_filename(product)}.pkl"
    with model_path.open("wb") as handle:
        pickle.dump(results, handle)

    return {
        "model_name": f"SARIMAX_{product}",
        "model_type": "sarimax",
        "product_type": df["tipo_produto"].dropna().unique().tolist()[0] if "tipo_produto" in df else None,
        "horizon": "12w",
        "metrics": metrics,
        "model_path": str(model_path),
    }

</code></pre>
<pre><code class='language-python'>def forecast_product(product: str, horizons: list[int], anchor_date: date | None = None) -&gt; pd.DataFrame:
    record = _load_latest_model(product)
    if record is None:
        raise RuntimeError(f"No trained model found for {product}.")

    model_path = Path(record["model_path"])
    last_point = _get_last_bbce_point(product)
    if last_point is None:
        raise RuntimeError(f"No BBCE history found for {product}.")
    last_date, last_price = last_point
    prev_points = _get_last_two_bbce_points(product)
    prev_price = prev_points[1][1] if prev_points else None

    rows = []
    model_type = record.get("model_type")
    base_date = anchor_date or last_date

    if model_type == "prophet":
        model = _load_model(model_path)
        max_weeks = max(horizons)
        future = model.make_future_dataframe(periods=max_weeks * 7)
        forecast = model.predict(future)

        for weeks in horizons:
            target_date = base_date + timedelta(days=weeks * 7)
            pred_row = forecast.loc[forecast["ds"] &gt;= pd.Timestamp(target_date)].head(1)
            if pred_row.empty:
                continue
            pred = pred_row.iloc[0]
            yhat = float(pred["yhat"])
            rows.append(
                {
                    "product": product,
                    "prediction_date": target_date,
                    "horizon": f"{weeks}w",
                    "predicted_value": yhat,
                    "confidence_interval_lower": float(pred["yhat_lower"]),
                    "confidence_interval_upper": float(pred["yhat_upper"]),
                    "trend": _trend_label(prev_price, last_price, yhat),
                    "model_id": record["id"],
                }
            )
    else:
        model = _load_sarimax_model(model_path)

        # Naive-last fallback saved by train_sarimax when data is too short.
        if isinstance(model, dict) and model.get("kind") == "naive_last":
            last = float(model.get("last") or 0.0)
            for weeks in horizons:
                target_date = base_date + timedelta(days=weeks * 7)
                rows.append(
                    {
                        "product": product,
                        "prediction_date": target_date,
                        "horizon": f"{weeks}w",
                        "predicted_value": last,
                        "confidence_interval_lower": last,
                        "confidence_interval_upper": last,
                        "trend": _trend_label(prev_price, last_price, last),
                        "model_id": record["id"],
                    }
                )
        else:
            # Model was trained on daily data; forecast in days and sample at the desired target dates.
            max_target = base_date + timedelta(days=max(horizons) * 7)
            max_steps = max(1, (max_target - last_date).days)
            forecast = model.get_forecast(steps=max_steps)
            mean = forecast.predicted_mean
            ci = forecast.conf_int(alpha=0.2)
            lower_col = ci.columns[0]
            upper_col = ci.columns[1]

            for weeks in horizons:
                target_date = base_date + timedelta(days=weeks * 7)
                steps_from_last = (target_date - last_date).days
                if steps_from_last &lt;= 0:
                    yhat = last_price
                    lower = last_price
                    upper = last_price
                else:
                    yhat = float(mean.iloc[steps_from_last - 1])
                    lower = float(ci[lower_col].iloc[steps_from_last - 1])
                    upper = float(ci[upper_col].iloc[steps_from_last - 1])
                rows.append(
                    {
                        "product": product,
                        "prediction_date": target_date,
                        "horizon": f"{weeks}w",
                        "predicted_value": yhat,
                        "confidence_interval_lower": lower,
                        "confidence_interval_upper": upper,
                        "trend": _trend_label(prev_price, last_price, yhat),
                        "model_id": record["id"],
                    }
                )

    return pd.DataFrame(rows)

</code></pre>
<p><span class='badge badge-impl'>Implementado</span> Prophet e SARIMAX. <span class='badge badge-plan'>Planejado</span> módulos adicionais (deep_learning/traditional/ensemble) sem fontes no repositório atual.</p>

<h3>src/visualization/</h3>
<p>Componentes e layouts do Streamlit, gráficos Plotly, mapas e tabelas de cenários.</p>
<ul>
  <li><code>components.py</code> concentra renderizações (cards, tabelas, gráficos, mapa).</li>
  <li><code>data.py</code> contém queries SQL para o dashboard.</li>
  <li><code>ui.py</code> injeta tema CSS e helpers de layout.</li>
</ul>

<h3>src/utils/</h3>
<p>Constantes de domínio: mapeamento de meses e prefixos de produto BBCE.</p>
<pre><code class='language-python'>MONTH_MAP = {
    "JAN": 1,
    "FEV": 2,
    "MAR": 3,
    "ABR": 4,
    "MAI": 5,
    "JUN": 6,
    "JUL": 7,
    "AGO": 8,
    "SET": 9,
    "OUT": 10,
    "NOV": 11,
    "DEZ": 12,
}

PRODUCT_TYPE_PREFIXES = {
    "ANU": "ANU",
    "SEM": "SEM",
    "TRI": "TRI",
    "MEN": "MEN",
}
</code></pre>

</div><div class='section' id='6-dashboard-documenta-o-completa'>
<h2>6. Dashboard - Documentação Completa</h2>

<p>Arquivo principal: <code>dashboard.py</code>. Fluxo:</p>

<ol>
  <li>Carrega semanas operativas e produtos BBCE.</li>
  <li>Roteia página de detalhe BBCE via query params (<code>bbce_view=product</code>).</li>
  <li>Renderiza cabeçalho + seletor de semana.</li>
  <li>Carrega dados BSO da semana selecionada.</li>
  <li>Exibe módulos de cenário energético, preços BBCE e evolução de geração/intercâmbio.</li>
</ol>

<p><strong>Observação:</strong> Módulo 1 de previsões BBCE está desativado no momento (conforme decisão do usuário).</p>
<pre><code class='language-text'>Arquivo: dashboard.py (trecho de fluxo principal)</code></pre>

</div><div class='section' id='7-pipeline-de-dados-completo'>
<h2>7. Pipeline de Dados Completo</h2>

<ol>
  <li><strong>Ingestão:</strong> <code>main.py --ingest</code> chama <code>run_ingestion()</code> que processa CSVs e escreve no banco.</li>
  <li><strong>Processamento:</strong> normalização, ajustes de Itaipu e intercâmbio do NE.</li>
  <li><strong>Persistência:</strong> upsert com <code>insert_dataframe</code>.</li>
  <li><strong>Treino:</strong> <code>--train</code> gera modelos Prophet/SARIMAX.</li>
  <li><strong>Previsão:</strong> <code>--predict</code> grava na tabela <code>predictions</code>.</li>
  <li><strong>Visualização:</strong> dashboard lê do banco via <code>src/visualization/data.py</code>.</li>
</ol>
<p>Pontos de falha: dados ausentes, tipos inválidos e valores NaN nos CSVs; o pipeline realiza coerção e remoção de valores inválidos.</p>

</div><div class='section' id='8-configura-o-e-deployment'>
<h2>8. Configuração e Deployment</h2>

<h3>Configurações</h3>
<ul>
  <li><code>.env</code> e <code>.env.example</code> definem conexão DB e paths.</li>
  <li><code>config/settings.py</code> centraliza defaults e monta URL do banco.</li>
  <li><code>.streamlit/config.toml</code> define porta/tema.</li>
</ul>
<pre><code class='language-bash'>DB_HOST=localhost
DB_PORT=5432
DB_NAME=energy_dashboard
DB_USER=dashboard_user
DB_PASSWORD=secure_password
# Optional override (useful for local SQLite validation):
# DATABASE_URL=sqlite:////home/capponni/Projeto_dashboard/project_root/data/database/energy_dashboard.db
DATA_RAW_DIR=/home/capponni/Projeto_dashboard/project_root/data/raw_csv
DATA_PROCESSED_DIR=/home/capponni/Projeto_dashboard/project_root/data/processed
MODEL_DIR=/home/capponni/Projeto_dashboard/project_root/data/models
</code></pre>
<pre><code class='language-python'>from __future__ import annotations

import os
from dataclasses import dataclass

from dotenv import load_dotenv


load_dotenv()


@dataclass(frozen=True)
class Settings:
    database_url_override: str | None = os.getenv("DATABASE_URL")
    db_host: str = os.getenv("DB_HOST", "localhost")
    db_port: int = int(os.getenv("DB_PORT", "5432"))
    db_name: str = os.getenv("DB_NAME", "energy_dashboard")
    db_user: str = os.getenv("DB_USER", "dashboard_user")
    db_password: str = os.getenv("DB_PASSWORD", "secure_password")

    data_raw_dir: str = os.getenv(
        "DATA_RAW_DIR",
        os.path.join(os.path.dirname(__file__), "..", "data", "raw_csv"),
    )
    data_processed_dir: str = os.getenv(
        "DATA_PROCESSED_DIR",
        os.path.join(os.path.dirname(__file__), "..", "data", "processed"),
    )

    model_dir: str = os.getenv(
        "MODEL_DIR",
        os.path.join(os.path.dirname(__file__), "..", "data", "models"),
    )

    @property
    def database_url(self) -&gt; str:
        if self.database_url_override:
            return self.database_url_override
        return (
            f"postgresql+psycopg2://{self.db_user}:{self.db_password}"
            f"@{self.db_host}:{self.db_port}/{self.db_name}"
        )


settings = Settings()
</code></pre>
<h3>Docker</h3>
<pre><code class='language-docker'>FROM python:3.10-slim

WORKDIR /app

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
    build-essential \
    gcc \
    g++ \
    libpq-dev \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . ./

ENV PYTHONUNBUFFERED=1
ENV STREAMLIT_SERVER_PORT=8501
ENV STREAMLIT_SERVER_ADDRESS=0.0.0.0

EXPOSE 8501

CMD ["streamlit", "run", "dashboard.py"]
</code></pre>
<pre><code class='language-yaml'>version: "3.9"

services:
  db:
    image: postgres:16
    restart: unless-stopped
    environment:
      POSTGRES_DB: energy_dashboard
      POSTGRES_USER: dashboard_user
      POSTGRES_PASSWORD: secure_password
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  app:
    build: .
    restart: unless-stopped
    environment:
      DB_HOST: db
      DB_PORT: 5432
      DB_NAME: energy_dashboard
      DB_USER: dashboard_user
      DB_PASSWORD: secure_password
    ports:
      - "8501:8501"
    depends_on:
      - db

volumes:
  pgdata:
</code></pre>

</div><div class='section' id='9-testes-e-valida-es'>
<h2>9. Testes e Validações</h2>

<p>Testes existentes em <code>tests/test_ingestion.py</code> cobrem:</p>
<ul>
  <li>Parsing de detalhes de produto BBCE.</li>
  <li>Validação de ajuste Itaipu no SE/CO.</li>
</ul>
<pre><code class='language-python'>import pandas as pd

from src.data_processing.ingestion import parse_product_details, process_bso


def test_parse_product_details():
    details = parse_product_details("ANU JAN/27 DEZ/27")
    assert details["tipo_produto"] == "ANU"
    assert details["mes_inicio"] == 1
    assert details["mes_fim"] == 12
    assert details["ano_referencia"] == 2027


def test_bso_adjustments(tmp_path):
    df = pd.DataFrame(
        {
            "bso": ["BSO - 01/01/2026 a 07/01/2026"],
            "submercado": ["Sudeste/Centro-Oeste"],
            "data_inicio": ["2026-01-01"],
            "data_fim": ["2026-01-07"],
            "carga": [100],
            "intercambio": [10],
            "hidraulica": [50],
            "termica": [20],
            "eolica": [10],
            "solar": [5],
            "ena": [90],
            "ear": [40],
            "itaipu": [15],
        }
    )
    path = tmp_path / "bso.csv"
    df.to_csv(path, index=False)

    processed = process_bso(path)
    assert processed.loc[0, "hidraulica"] == 65
</code></pre>
<p><span class='badge badge-plan'>Planejado</span> testes adicionais indicados na documentação: intercâmbio, navegação semanal, etc.</p>

</div><div class='section' id='10-an-lise-de-dados-do-mercado-de-energia'>
<h2>10. Análise de Dados do Mercado de Energia</h2>

<p>Fontes de dados utilizadas:</p>
<ul>
  <li><strong>BSO (ONS):</strong> dados operativos semanais (carga, geração por fonte, ENA/EAR).</li>
  <li><strong>BBCE:</strong> preços de produtos ANU/SEM/TRI/MEN.</li>
  <li><strong>PLD/CMO:</strong> indicadores econômicos operativos por região.</li>
  <li><strong>Bandeiras tarifárias:</strong> ban.csv para identificação do adicional mensal.</li>
</ul>
<p>Transformações de domínio principais: soma de Itaipu em hidro SE/CO e inversão de sinal do intercâmbio do Nordeste.</p>

</div><div class='section' id='11-modelos-de-machine-learning'>
<h2>11. Modelos de Machine Learning</h2>

<p>Modelos implementados:</p>
<ul>
  <li><strong>Prophet</strong> (se disponível no ambiente): sazonalidade semanal/anual.</li>
  <li><strong>SARIMAX</strong> (statsmodels): baseline diário com sazonalidade semanal.</li>
</ul>
<p>Métricas calculadas: MAE, RMSE, MAPE (armazenadas em <code>ml_models.metrics</code>).</p>
<p><span class='badge badge-plan'>Planejado</span> modelos adicionais (Random Forest, XGBoost, LSTM etc.) referenciados na documentação <code>docs/model_docs.md</code>, mas não presentes no código atual.</p>
<pre><code class='language-markdown'># Model Documentation

## Baseline Model: Prophet
- Objective: BBCE price forecasting for 1/4/12 week horizons
- Inputs: Historical BBCE prices
- Metrics: MAE, RMSE, MAPE (stored in `ml_models.metrics`)

## Future Models
- Random Forest Regressor with energy indicators as regressors
- XGBoost for higher-capacity price modeling
- SARIMAX with exogenous variables

## Retraining
Scheduled weekly (Sunday 02:00) in production deployments.
</code></pre>

</div><div class='section' id='12-considera-es-t-cnicas-para-tcc'>
<h2>12. Considerações Técnicas para TCC</h2>

<ul>
  <li><strong>Inovações:</strong> integração de dados do setor elétrico com dashboard interativo e pipeline completo de ingestão.</li>
  <li><strong>Contribuições:</strong> consolidação de indicadores operativos + preços BBCE em uma única interface.</li>
  <li><strong>Dificuldades técnicas:</strong> normalização de dados BBCE (escala 10x), coerção de datas e robustez do SQLite.</li>
  <li><strong>Lições aprendidas:</strong> importância de validação de dados e padronização de séries temporais.</li>
  <li><strong>Trabalhos futuros:</strong> ampliar ML multivariado e incluir modelos avançados (deep learning/ensemble).</li>
</ul>

</div><div class='section' id='13-gloss-rio'>
<h2>13. Glossário</h2>

<table class='table'>
  <tr><th>Termo</th><th>Definição</th></tr>
  <tr><td>BSO</td><td>Boletim Semanal da Operação (ONS).</td></tr>
  <tr><td>BBCE</td><td>Balcão Brasileiro de Comercialização de Energia.</td></tr>
  <tr><td>PLD</td><td>Preço de Liquidação das Diferenças.</td></tr>
  <tr><td>CMO</td><td>Custo Marginal de Operação.</td></tr>
  <tr><td>ENA/EAR</td><td>Energia Natural Afluente / Energia Armazenada.</td></tr>
  <tr><td>SARIMAX</td><td>Modelo ARIMA com componentes sazonais e regressoras.</td></tr>
</table>

</div><div class='section' id='14-refer-ncias'>
<h2>14. Referências</h2>

<ul>
  <li>Repositório local e arquivos analisados: <code>/home/capponni/Projeto_dashboard/project_root</code>.</li>
  <li>Bibliotecas declaradas em <code>requirements.txt</code>:</li>
</ul>
<pre><code class='language-text'>streamlit&gt;=1.30
pandas&gt;=2.0
numpy&gt;=1.24
sqlalchemy&gt;=2.0
psycopg2-binary&gt;=2.9
python-dotenv&gt;=1.0
scikit-learn&gt;=1.3
statsmodels&gt;=0.14
prophet&gt;=1.1
plotly&gt;=5.18
pytest&gt;=7.4
streamlit-plotly-events&gt;=0.0.6
streamlit-javascript&gt;=0.1.5</code></pre>
<p>Fontes de dados mencionadas: ONS, CCEE, BBCE (conforme cabeçalho e documentação do projeto).</p>

</div><div class='footer'>Relatório gerado automaticamente para fins de TCC. Nenhuma informação foi inferida fora do repositório atual.</div></div></body></html>